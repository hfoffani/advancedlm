---
title: "Code Examples"
author: "herchu1"
date: "17 May 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Code Examples

R Markdown see <http://rmarkdown.rstudio.com>.

Setup **code**:

```{r}
data("mtcars")
head(mtcars)
y = mtcars$mpg
x = cbind(1, mtcars$wt, mtcars$hp)
head(x)
```

### Least squares.

Fit with least squares.

$$ (x' * x)^{-1} * x' * y $$

```{r}
solve( t(x) %*% x ) %*% t(x) %*% y
```

R does the same using `lm`

```{r}
coef( lm( mpg ~ wt + hp, data=mtcars ))
```


### Mean centered

A matrix of all-ones is often named as $J$. So given

$$ H = J_n * (J_n' * J_n)^{-1} * J_n' $$
$H$ results on an $n * n$ matrix of all-ones divided by $n$.

```{r}
n = nrow(x)
v1 = matrix(1, n)
H = v1 %*% solve(t(v1) %*% v1) %*% t(v1)
all( H == matrix(1, n, n) / n )
```

Centering `x` to the mean is

$$ (I - H) * x $$

```{r}
I = diag(1, n)
H = matrix(1, n, n) / n

xc = (I - H) %*% x
```

Testing the result. Take the mean of all columns (axis 2).
Using `sweep` can be faster.

```{r}
apply(xc, 2, mean)
xc2 = sweep(x, 2, apply(x, 2, mean))
```

### Variance

Using linear algebra and comparing it with bundled R function `var`.

$$ Variance = \frac{ x' * (I - H) * x }{n-1} $$

```{r}
vx = ( t(x) %*% (I - H) %*% x ) / (n-1)

round(vx, 6)
var(x)
```


### Regression through the origin

The slope of the fit is

$$ \frac{ \tilde{y} \cdot \tilde{x}  }{ \tilde{x} \cdot \tilde{x}  } $$

where $\tilde{y}$ is $y$ mean centered. Same for $\tilde{x}$

Also the slope is (where $\rho_{xy}$ is the covariance between $x$ and $y$)

$$ \rho_{xy} \frac{\sigma_y}{\sigma_x} $$


```{r}
x = mtcars$wt
y = mtcars$disp

yc = y - mean(y)
xc = x - mean(x)
slope = sum( yc * xc ) / sum( xc * xc )
slope
lm(formula = yc ~ xc - 1)
cor(x,y) * sd(y) / sd(x)
```


### Fitting a linear regresion

```{r, echo=FALSE}
plot(x, y)
abline(lm(y ~ x))
```
```{r}
lm(y ~ x)
```

Again the slope $\beta_1$ is:

$$ \rho_{xy} \frac{\sigma_y}{\sigma_x} $$

And the intercept $\beta_0$ is:

$$ \tilde{y} - \rho_{xy} \frac{\sigma_y}{\sigma_x} \tilde{x} $$

```{r}
cor(x,y) * sd(y) / sd(x)
mean(y) - cor(x,y) * sd(y) / sd(x) * mean(x)
```

Same thing as before with the mean centered values:

```{r}
sum(yc * xc) / sum(xc^2)
```


### Fitted values and residuals

The fitted values are

$$ \hat{y} = \beta_0 * J_n + \beta_1 * x $$

and the residuals

$$ e = y - \hat{y} $$

```{r}
Jn = matrix(1, length(x))
beta1 = cor(x,y) * sd(y) / sd(x)
beta0 = mean(y) - beta1 * mean(x)
beta0
beta1
yhat = beta0 * Jn + beta1 * x
e = y - yhat
```

*In R, Jn is not needed as it promotes scalars to vector automatically.*

```{r, echo=FALSE}
plot(x, e)
abline(h=0)
```


### Least squares

Minimize the square of the norm of the residuals

$$ \| { y - \hat{y} } \| ^2 $$

```{r}
sum(e^2)
y_hat_other_beta = beta0 * Jn + 120 * x
sum((y - y_hat_other_beta)^2)
```

It's a worst solution.


### Prediction

```{r, echo=F}
areequal <- function (a, b) {
    all( abs(a - b) < 0.00001 )
}
```

```{r}
pred = predict(lm(y ~ x))
areequal(yhat, pred)
```


### Residuals

```{r}
resd = resid(lm(y ~ x))
areequal(e, resd)
```


### Generalization

If we have
$$
    \begin{aligned}
    y(t) &= t + 2 t^2   \\
    x(t) &= t           \\
    \end{aligned}
$$
Minimizing $\| y - \beta x \|^2$ we get
$$
    \begin{aligned}
    \beta &= \frac{\langle y,x \rangle}{\langle x,x \rangle}   \\
          &= \frac{\int_0^1 (t+2t^2)t \,dt}{\int_0^1 t^2 \,dt}    \\
          &= \frac{\frac{1}{3} + \frac{1}{2}}{\frac{1}{3}}      \\
          &= 2.5                                    \\
    \end{aligned}
$$

```{r}
t = seq(0, 1, length=10000)
y = t + 2 * t ^ 2
x = t
coef(lm(y ~ x - 1))
```


### Least Squares on Multivariate

Same as single predictor but with matrices.

$$
\hat{\beta} = (X'X)^{-1}X'Y
$$

With `swiss` dataset predict `Fertility` using the other variables.

```{r}
data(swiss)
x = cbind(1, swiss[,-1])
y = swiss$Fertility
x = as.matrix(x)
betahat = solve(t(x) %*% x) %*% t(x) %*% y
betahat
```

Passing to `solve` all the equations is computationally faster.

```{r}
solve(t(x) %*% x, t(x) %*% y)
```


### Coefficients as being adjusted for the other variables.

Adjust the outcome and the other variables and obtain each of the coefficients.

1. Pick one variable.
2. Explain the outcome given the rest of the variables and obtain the residuals.
3. Explain the variable 1 given the others variables and obtain the residuals.

Fitting the residuals of 3 given the residuals of 2 gives the coefficient for the variable 1.

```{r}
x1 = x[,1:3]
x2 = x[,4:6]
ey = y - x1 %*% solve(t(x1) %*% x1) %*% t(x1) %*% y
ex2 = x2 - x1 %*% solve(t(x1) %*% x1) %*% t(x1) %*% x2
solve(t(ex2) %*% ex2) %*% t(ex2) %*% ey
```



### Bases.

Example with Principal Components

```{r}
x = as.matrix(swiss[,-1])
y = swiss$Fertility
n = nrow(x)
```

R provides a function. With `cor=T` variables in different units are comparable.

```{r}
decomp = princomp(x, cor=T)
names(decomp)
plot(cumsum(decomp$sdev^2)/sum(decomp$sdev^2), type="l")
```

Another way is to take the eigen values and vectors from the correlation matrix.

```{r}
decomp2 = eigen(cor(x))
names(decomp2)
```

The eigen vectors are orthonormal.

```{r}
all( t(decomp2$vectors) %*% decomp2$vectors - diag(5) < 0.0001)
```

An alternative way is by using Single Value Decomposition SVD.
First we need to normalize the design matrix.

```{r}
xnorm = apply(x, 2, function(z) (z - mean(z)) / sd(z))
decomp3 = svd(xnorm)
names(decomp3)
```

All three are equivalent. First we compare the eigen vectors, then the eigen values.

```{r}
round(rbind(decomp2$vectors, decomp$loadings, decomp3$v), 3)
round(rbind(decomp2$values, decomp$sdev^2, decomp3$d^2/(n-1)), 3)
```

Comparing `u` matrix with eigen vectors and eigen values.

```{r}
plot(
    decomp3$u[,1],
    xnorm %*% decomp2$vectors %*% diag(1/sqrt(decomp2$values))[,1]
)
```

Fixing a smaller model

```{r}
u = decomp3$u[,1:4]
summary(lm(y ~ u))$r.squared
summary(lm(y ~ x))$r.squared
```

Because matrix `u` is orthonormal the coefficients are just:

```{r}
lm(y ~ u)$coeff
t(u) %*% y
```


### Residuals

Useful formulas

$$
e = y - \hat{y}
$$

$$
H_X = X(X'X)^{-1}X'
$$

$$
e = (I - H_X) y
$$

$$
SS_{Tot} = y'(I - H_J)y
$$

$$
SS_{Res} = y'(I - H_X)y
$$

$$
SS_{Reg} = y'(H_X - H_J)y
$$

$$
SS_{Tot} = SS_{Res} + SS_{Reg}
$$

$$
R^2 = \frac{SS_{Reg}}{SS_{Tot}}
$$
